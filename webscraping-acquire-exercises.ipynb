{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7bb74e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from acquire import get_inshorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34ea91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make request to 'https://web-scraping-demo.zgulde.net/people'\n",
    "response = requests.get('https://web-scraping-demo.zgulde.net/people', headers={'user-agent': 'Codeup DS Hoppper'})\n",
    "# Use BeautifulSoup to store response content.\n",
    "soup = BeautifulSoup(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279e3869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check contents make sure it is HTML data.\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ff2c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the element from the proper tag.\n",
    "people = soup.select('.person')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34573ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the contents.\n",
    "people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bad0fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the first item in the list.\n",
    "person = people[0]\n",
    "person"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afd1b9d",
   "metadata": {},
   "source": [
    "#### Save wanted values into variables to be accessed later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67afcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = person.h2.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8221fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "quote = person.p.text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37aabba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "email = person.select('.email')[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bea3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "phone = person.select('.phone')[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ebc0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "person.select('p')[-1].text.strip().replace('\\n',',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18664468",
   "metadata": {},
   "outputs": [],
   "source": [
    "address = [l.strip() for l in person.select('p')[-1].text.split('\\n')[1:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fb8b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will create a dictionary of the wanted data.\n",
    "def parse_person(person):\n",
    "    name = person.h2.text\n",
    "    quote = person.p.text.strip()\n",
    "    email = person.select('.email')[0].text\n",
    "    phone = person.select('.phone')[0].text\n",
    "    address = [l.strip() for l in person.select('p')[-1].text.split('\\n')[1:3]]\n",
    "\n",
    "  \n",
    "    return {\n",
    "        'name': name, 'quote': quote, 'email': email,\n",
    "        'phone': phone,\n",
    "        'address': address\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035140c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_person(person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983da9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through all the articles\n",
    "people_df = pd.DataFrame([parse_person(person) for person in people])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cd4640",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_df['street_adress'] = people_df.address[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc4ad6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_df['city_state'] = people_df.address[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7edb9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_df.rename(columns={'city_state':'city_state_zip'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a2319c",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0b41df",
   "metadata": {},
   "source": [
    "#### Codeup Blog Articles\n",
    "\n",
    "Visit Codeup's Blog(http://codeup.com/blog/) and record the urls for at least 5 distinct blog posts. For each post, you should scrape at least the post's title and content.\n",
    "\n",
    "Encapsulate your work in a function named get_blog_articles that will return a list of dictionaries, with each dictionary representing one article. The shape of each dictionary should look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4758e544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save request response into a variable\n",
    "response = requests.get('https://codeup.com/blog/', headers={'user-agent': 'Codeup DS Hopper'})\n",
    "# Store response content\n",
    "soup = BeautifulSoup(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87254e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify HTML data.\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721df1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('a',class_ = 'more-link')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28473a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(soup.find_all('a',class_ = 'more-link'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6144b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(soup.find_all('a',class_ = 'more-link'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29caf214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will store a list of urls from Codeup's 'Codeup News & Articles' page.\n",
    "def get_urls(soup):\n",
    "    urls = soup.find_all('a',class_ = 'more-link')\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc0d6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = get_urls(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1077e2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46a62ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access first attributes from list.\n",
    "first_blog = urls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57311a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_blog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3c2d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store url by accessing 'href' attribute from 'a' anchor tag.\n",
    "first_url = first_blog.get('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e850ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2146227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store response from request of the first url\n",
    "response_blog = requests.get(first_url, headers={'user-agent': 'Codeup DS Hopper'})\n",
    "response_blog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0279ad5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store contents of response.\n",
    "blog = BeautifulSoup(response_blog.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e86c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "blog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177720ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use soup to get title of blog.\n",
    "title = blog.h1.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bb3384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify contents of title.\n",
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19352ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use soup to get date and source of blog.\n",
    "date_source = blog.p.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edecb7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify contents.\n",
    "date_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208477a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use soup to get the contents of the blog.\n",
    "content = blog.find_all('div',class_ = 'entry-content')[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51498396",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Verify content.\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ebcfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes in a url as an argument and parse the contents.\n",
    "def parse_blog(url):\n",
    "    url = url.get('href')\n",
    "    response = requests.get(url, headers={'user-agent': 'Codeup DS Hopper'})\n",
    "    blog = BeautifulSoup(response.text)\n",
    "    title = blog.h1.text\n",
    "    date_source = blog.p.text\n",
    "    content = blog.find_all('div',class_ = 'entry-content')[0].text\n",
    "      \n",
    "    return {\n",
    "        'title': title, 'date & source': date_source, 'content': content\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ad83d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will loop through a list of urls and return a dataframe.\n",
    "def get_codeup_blogs():\n",
    "    blog_df = pd.DataFrame([parse_blog(url) for url in urls])\n",
    "    return blog_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf00f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = get_codeup_blogs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139b8e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb746b5",
   "metadata": {},
   "source": [
    "# inshorts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd6f942",
   "metadata": {},
   "source": [
    "We will now be scraping text data from inshorts, a website that provides a brief overview of many different topics.\n",
    "\n",
    "Write a function that scrapes the news articles for the following topics:\n",
    "\n",
    "* Business\n",
    "* Sports\n",
    "* Technology\n",
    "* Entertainment\n",
    "\n",
    "\n",
    "The end product of this should be a function named get_news_articles that returns a list of dictionaries, where each dictionary has this shape:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f918fb",
   "metadata": {},
   "source": [
    "{\n",
    "    'title': 'The article title',\n",
    "    'content': 'The article content',\n",
    "    'category': 'business' # for example\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e81a206",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = ['https://inshorts.com/en/read/science', 'https://inshorts.com/en/read/business','https://inshorts.com/en/read/sports','https://inshorts.com/en/read/technology','https://inshorts.com/en/read/entertainment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e228b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5b5152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make request to 'https://inshorts.com/en/read/science'\n",
    "response = requests.get(urls[0])\n",
    "# Use BeautifulSoup to store response content.\n",
    "soup = BeautifulSoup(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf330b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cards = soup.find_all('div', class_='news-card')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69faf1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e59fca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [card.find('span', itemprop = 'headline').text for card in cards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69789c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706bb1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = [card.find('span', class_='author').text for card in cards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c0f2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2544bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = [card.find('div', itemprop='articleBody').text for card in cards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1423ba62",
   "metadata": {},
   "outputs": [],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcaf486",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = ['https://inshorts.com/en/read/science', 'https://inshorts.com/en/read/business','https://inshorts.com/en/read/sports','https://inshorts.com/en/read/technology','https://inshorts.com/en/read/entertainment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f9a0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list, articles, to hold the dictionaries for each article.\n",
    "articles = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a781cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in urls:\n",
    "    # Make request to 'https://inshorts.com/en/read/science'\n",
    "    response = requests.get(url)\n",
    "    # Use BeautifulSoup to store response content.\n",
    "    soup = BeautifulSoup(response.text)\n",
    "    cards = soup.find_all('div', class_='news-card')\n",
    "    # Loop through each news card on the page and get what we want\n",
    "    for card in cards:\n",
    "        title = card.find('span', itemprop='headline' ).text\n",
    "        author = card.find('span', class_='author').text\n",
    "        content = card.find('div', itemprop='articleBody').text\n",
    "        category = url[29:]\n",
    "        \n",
    "        # Create a dictionary, article, for each news card\n",
    "        article = {'title': title, 'category': category, 'author': author, 'content': content}\n",
    "        \n",
    "        # Add the dictionary, article, to our list of dictionaries, articles.\n",
    "        articles.append(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a437c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1de916d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brent/codeup-data-science/natural-language-processing-exercises/acquire.py:79: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 79 of the file /Users/brent/codeup-data-science/natural-language-processing-exercises/acquire.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  soup = BeautifulSoup(response.text)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Omicron BA.2 found in 57 countries, doesn't se...</td>\n",
       "      <td>science</td>\n",
       "      <td>Apaar Sharma</td>\n",
       "      <td>Omicron BA.2 variant has been found in 57 coun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Satellite images show the world's longest 768-...</td>\n",
       "      <td>science</td>\n",
       "      <td>Pragya Swastik</td>\n",
       "      <td>Satellite images have captured the 768-km-long...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.9 crore-year-old flowers found perfectly pre...</td>\n",
       "      <td>science</td>\n",
       "      <td>Ankush Verma</td>\n",
       "      <td>Two 9.9 crore-year-old flowers have been found...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Astronaut shares pics of clouds taken from spa...</td>\n",
       "      <td>science</td>\n",
       "      <td>Daisy Mowke</td>\n",
       "      <td>Astronaut Kayla Barron, who is currently aboar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Picture of Mars crater that looks like a tree ...</td>\n",
       "      <td>science</td>\n",
       "      <td>Pragya Swastik</td>\n",
       "      <td>The European Space Agency (ESA) has released a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>Not my style: Deepika on being asked if she ga...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>Mahima Kharbanda</td>\n",
       "      <td>When asked if she had given any special dating...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>Brotherhood created with Tom, Tobey over diffi...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>Kriti Kambiri</td>\n",
       "      <td>Actor Andrew Garfield revealed that a brotherh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>Court directs Honey Singh to give voice sample...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>Kriti Kambiri</td>\n",
       "      <td>Rapper Yo Yo Honey Singh has been directed by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>Javed confirms Farhan-Shibani's wedding, will ...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>Kriti Kambiri</td>\n",
       "      <td>Lyricist Javed Akhtar has confirmed actor Farh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>Tejasswi winning Bigg Boss because of Naagin i...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>Kriti Kambiri</td>\n",
       "      <td>Actor Karan Kundrra has said that claims of hi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title       category  \\\n",
       "0    Omicron BA.2 found in 57 countries, doesn't se...        science   \n",
       "1    Satellite images show the world's longest 768-...        science   \n",
       "2    9.9 crore-year-old flowers found perfectly pre...        science   \n",
       "3    Astronaut shares pics of clouds taken from spa...        science   \n",
       "4    Picture of Mars crater that looks like a tree ...        science   \n",
       "..                                                 ...            ...   \n",
       "120  Not my style: Deepika on being asked if she ga...  entertainment   \n",
       "121  Brotherhood created with Tom, Tobey over diffi...  entertainment   \n",
       "122  Court directs Honey Singh to give voice sample...  entertainment   \n",
       "123  Javed confirms Farhan-Shibani's wedding, will ...  entertainment   \n",
       "124  Tejasswi winning Bigg Boss because of Naagin i...  entertainment   \n",
       "\n",
       "               author                                            content  \n",
       "0        Apaar Sharma  Omicron BA.2 variant has been found in 57 coun...  \n",
       "1      Pragya Swastik  Satellite images have captured the 768-km-long...  \n",
       "2        Ankush Verma  Two 9.9 crore-year-old flowers have been found...  \n",
       "3         Daisy Mowke  Astronaut Kayla Barron, who is currently aboar...  \n",
       "4      Pragya Swastik  The European Space Agency (ESA) has released a...  \n",
       "..                ...                                                ...  \n",
       "120  Mahima Kharbanda  When asked if she had given any special dating...  \n",
       "121     Kriti Kambiri  Actor Andrew Garfield revealed that a brotherh...  \n",
       "122     Kriti Kambiri  Rapper Yo Yo Honey Singh has been directed by ...  \n",
       "123     Kriti Kambiri  Lyricist Javed Akhtar has confirmed actor Farh...  \n",
       "124     Kriti Kambiri  Actor Karan Kundrra has said that claims of hi...  \n",
       "\n",
       "[125 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_inshorts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de0ad43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
