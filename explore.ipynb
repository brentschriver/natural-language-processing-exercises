{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279ef8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import acquire\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "from acquire import parse_blog\n",
    "from acquire import get_article_text\n",
    "from acquire import get_codeup_blogs\n",
    "from acquire import get_inshorts_articles\n",
    "from acquire import prep_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf455f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store contents of 'spam.csv' into a variable\n",
    "spam = pd.read_csv('spam.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd06b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check column names.\n",
    "spam.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207175fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unwanted columns.\n",
    "spam.drop(columns=['Unnamed: 2','Unnamed: 3','Unnamed: 4'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166fd0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns.\n",
    "spam.rename(columns={'v1':'label','v2':'text'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6eb3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9d25a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add words to append to list of stopwords.\n",
    "ADDITIONAL_STOPWORDS = ['r','u','2','ltgt']\n",
    "\n",
    "# This function will append list of words to stopwords\n",
    "def clean(text):\n",
    "    # Create lemmatizer object\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    # Create list of stopwords\n",
    "    stopwords = nltk.corpus.stopwords.words('english') + ADDITIONAL_STOPWORDS\n",
    "    # Convert string to ASCII character set.\n",
    "    text = (unicodedata.normalize('NFKD', text)\n",
    "           .encode('ascii','ignore')\n",
    "           .decode('utf-8','ignore')\n",
    "           .lower())\n",
    "    # Remove special characters.\n",
    "    words = re.sub(r'[^\\w\\s]', '', text).split()\n",
    "    return [wnl.lemmatize(word) for word in words if word not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcff6cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function shows number of words and the ratio.\n",
    "def show_counts_and_ratios(df, column):\n",
    "    labels = pd.concat([spam.label.value_counts(),\n",
    "                       spam.label.value_counts(normalize=True)], axis =1)\n",
    "    labels.columns = ['n', 'percent']\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615f0f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_counts_and_ratios(spam, 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4c5522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataframe into text.\n",
    "ham_words = clean(' '.join(spam[spam.label == 'ham'].text))\n",
    "spam_words = clean(' '.join(spam[spam.label == 'spam'].text))\n",
    "all_words = clean(' '.join(spam.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69986e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check top 5 words in each list.\n",
    "ham_words[:5], spam_words[:5], all_words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6b9b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get value counts of each word in each list.\n",
    "ham_freq = pd.Series(ham_words).value_counts()\n",
    "spam_freq = pd.Series(spam_words).value_counts()\n",
    "all_freq = pd.Series(all_words).value_counts()\n",
    "\n",
    "spam_freq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a4fee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([all_freq, ham_freq, spam_freq], axis=1, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8836f8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine value counts of words into one dataframe to work on\n",
    "\n",
    "word_counts = (pd.concat([all_freq, ham_freq, spam_freq], axis=1, sort=True)\n",
    "              .set_axis(['all', 'ham', 'spam'], axis=1, inplace=False)\n",
    "              .fillna(0) \n",
    "              .apply(lambda s: s.astype(int)))\n",
    "\n",
    "word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e8e65d",
   "metadata": {},
   "source": [
    "### Are there any words that uniquely identify a spam or ham message?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329f7bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([word_counts[word_counts.ham == 0].sort_values(by='spam', ascending=False).head(10),\n",
    "          word_counts[word_counts.spam == 0].sort_values(by='ham', ascending=False).head(10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843e198d",
   "metadata": {},
   "source": [
    "### What are the most frequently occuring words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2760bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts.sort_values(by='all', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca22725",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23538d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f051cc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ratios of spam and ham words.\n",
    "(word_counts\n",
    ".assign(p_spam=word_counts.spam / word_counts['all'],\n",
    "        p_ham=word_counts.ham / word_counts['all'])\n",
    ".sort_values(by='all')\n",
    " [['p_spam', 'p_ham']]\n",
    " .tail(20)\n",
    " .sort_values('p_ham')\n",
    " .plot.barh(stacked=True))\n",
    "\n",
    "plt.title('Proportion of Spam vs Ham for the 20most common words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de882a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get values and ratios of all words that have a value count greater than 10.\n",
    "(word_counts\n",
    "[(word_counts.spam > 10) & (word_counts.ham > 10)]\n",
    ".assign(ratio=lambda df: df.spam / (df.ham + .01))\n",
    ".sort_values(by='ratio')\n",
    ".pipe(lambda df: pd.concat([df.head(), df.tail()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4abb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Create sentence for wordcloud\n",
    "sentence = \"Mary had a little lamb, little lamb, little lamb. Its fleece was white as snow.\"\n",
    "\n",
    "# Generate the image using previously stored sentence.\n",
    "img = WordCloud(background_color='white').generate(sentence)\n",
    "\n",
    "# Display image.\n",
    "plt.imshow(img)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d373ca71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate images from joining all words in the list into one 'sentence' using '.join'\n",
    "all_cloud = WordCloud(background_color='white', height=1000, width=400).generate(' '.join(all_words))\n",
    "ham_cloud = WordCloud(background_color='white', height=600, width=800).generate(' '.join(ham_words))\n",
    "spam_cloud = WordCloud(background_color='white', height=600, width=800).generate(' '.join(spam_words))\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "axs = [plt.axes([0,0,.5,1]), plt.axes([.5,.5,.5,.5]), plt.axes([.5,0,.5,.5])]\n",
    "\n",
    "axs[0].imshow(all_cloud)\n",
    "axs[1].imshow(ham_cloud)\n",
    "axs[2].imshow(spam_cloud)\n",
    "\n",
    "axs[0].set_title('All Words')\n",
    "axs[1].set_title('Ham')\n",
    "axs[2].set_title('Spam')\n",
    "\n",
    "for ax in axs: ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dfc28d",
   "metadata": {},
   "source": [
    "## Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d59c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Mary had a little lamb\"\n",
    "\n",
    "bigrams = nltk.ngrams(sentence.split(), 2)\n",
    "list(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d6df5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_ham_bigrams = (pd.Series(nltk.ngrams(ham_words, 2))\n",
    "                     .value_counts()\n",
    "                     .head(20))\n",
    "\n",
    "top_20_ham_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294b3f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_ham_bigrams.reset_index()['index'].apply(lambda t: t[0] + ' ' + t[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1572ff47",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_ham_bigrams.plot.barh(color='pink', width=.9, figsize=(10,6))\n",
    "\n",
    "plt.title('20 Most Frequently Occuring Ham Bigrams')\n",
    "plt.ylabel('Bigram')\n",
    "plt.xlabel('Number of Occurances')\n",
    "\n",
    "ticks , _ = plt.yticks()\n",
    "labels = top_20_ham_bigrams.reset_index()['index'].apply(lambda t: t[0] + ' ' + t[1])\n",
    "_ = plt.yticks(ticks, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f9562e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {k[0] + ' ' + k[1]: v for k, v in top_20_ham_bigrams.to_dict().items()}\n",
    "img = WordCloud(background_color='white', width=800, height=400).generate_from_frequencies(data)\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49597f21",
   "metadata": {},
   "source": [
    "## Create and explore bigrams for the spam data. Visualize them with a word cloud. How do they compare with the ham bigrams?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81bb45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_spam_bigrams = (pd.Series(nltk.ngrams(spam_words, 2))\n",
    "                      .value_counts()\n",
    "                      .head(20))\n",
    "\n",
    "top_20_spam_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a641120e",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_spam_bigrams.plot.barh(color='blue', width=.9, figsize=(10,6))\n",
    "\n",
    "plt.title('20 Most Frequently Occuring Spam Bigrams')\n",
    "plt.ylabel('Bigram')\n",
    "plt.xlabel('Number of Occurances')\n",
    "\n",
    "# Make the labels pretty\n",
    "ticks, _ = plt.yticks()\n",
    "labels = top_20_spam_bigrams.reset_index()['index'].apply(lambda t: t[0] + ' ' + t[1])\n",
    "_ = plt.yticks(ticks, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca2727a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {k[0] + ' ' + k[1]: v for k, v in top_20_spam_bigrams.to_dict().items()}\n",
    "img = WordCloud(background_color='white', width=800, height=400).generate_from_frequencies(data)\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2007bf0",
   "metadata": {},
   "source": [
    "## Create and explore with trigrams(i.e. an n-gram with an n of 3) for both the spam and ham data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938cc587",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_ham_trigrams = (pd.Series(nltk.ngrams(ham_words, 3))\n",
    "                     .value_counts()\n",
    "                     .head(20))\n",
    "\n",
    "top_20_ham_trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cc8d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_ham_trigrams.plot.barh(color='pink', width=.9, figsize=(10,6))\n",
    "\n",
    "plt.title('20 Most Frequently Occuring Ham Trigrams')\n",
    "plt.ylabel('Trigram')\n",
    "plt.xlabel('Number of Occurances')\n",
    "\n",
    "ticks, _ = plt.yticks()\n",
    "labels = top_20_ham_trigrams.reset_index()['index'].apply(lambda t: t[0] + ' ' + t[1] + ' ' + t[2])\n",
    "_ = plt.yticks(ticks, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7897f4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {k[0] + ' ' + k[1] + ' ' + k[2]: v for k, v in top_20_ham_trigrams.to_dict().items()}\n",
    "img = WordCloud(background_color='white', width=800, height=400).generate_from_frequencies(data)\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f9047c",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_spam_trigrams = (pd.Series(nltk.ngrams(spam_words, 3))\n",
    "                      .value_counts()\n",
    "                      .head(20))\n",
    "\n",
    "top_20_spam_trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5f096b",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_spam_trigrams.plot.barh(color='pink', width=.9, figsize=(10,6))\n",
    "\n",
    "plt.title('20 Most Frequently Occuring Spam Trigrams')\n",
    "plt.ylabel('Trigram')\n",
    "plt.xlabel('Number of Occurances')\n",
    "\n",
    "ticks, _ = plt.yticks()\n",
    "labels = top_20_spam_trigrams.reset_index()['index'].apply(lambda t: t[0] + ' ' + t[1] + ' ' + t[2])\n",
    "_ = plt.yticks(ticks, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6a7b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {k[0] + ' ' + k[1] + ' ' + k[2]: v for k, v in top_20_spam_trigrams.to_dict().items()}\n",
    "img = WordCloud(background_color='white', width=800, height=400).generate_from_frequencies(data)\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157e5893",
   "metadata": {},
   "outputs": [],
   "source": [
    "codeup_df = acquire.get_codeup_blogs(cached=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6009e142",
   "metadata": {},
   "outputs": [],
   "source": [
    "codeup_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21749517",
   "metadata": {},
   "outputs": [],
   "source": [
    "acquire.prep_text(codeup_df, 'original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22a22e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = get_inshorts_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c27068",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883e00c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_text(news_df, 'original', extra_words=[\"'\", ','])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f2cd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.original[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce172922",
   "metadata": {},
   "source": [
    "## Create a separate dataframe for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8c5d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71255eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_dfs = []\n",
    "for cat in news_df.category.unique():\n",
    "    list_of_dfs.append(news_df[news_df.category == cat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f990fe5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "news_df.clean[0:21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0c2e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.lemmatized[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba78ac31",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.stemmed[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00d753f",
   "metadata": {},
   "source": [
    "# Takeaways\n",
    "* There is not much difference between the lemmatized and clean versions. The stemmed version is difficult for me to understand. \n",
    "* I will work on the clean version of text for exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7e3adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.category.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4311e60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a corpus of words for each category.\n",
    "science_words = ' '.join(news_df[news_df.category == 'science'].clean)\n",
    "business_words = ' '.join(news_df[news_df.category == 'business'].clean)\n",
    "sports_words = ' '.join(news_df[news_df.category == 'sports'].clean)\n",
    "technology_words = ' '.join(news_df[news_df.category == 'technology'].clean)\n",
    "entertainment_words = ' '.join(news_df[news_df.category == 'entertainment'].clean)\n",
    "all_words = ' '.join(news_df.clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99057665",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(science_words), len(business_words), len(sports_words), len(technology_words), len(entertainment_words), len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49665314",
   "metadata": {},
   "outputs": [],
   "source": [
    "science_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dc4691",
   "metadata": {},
   "source": [
    "### Represent text as word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62dbe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "science_freq = pd.Series(science_words.split()).value_counts()\n",
    "business_freq = pd.Series(business_words.split()).value_counts()\n",
    "sports_freq = pd.Series(sports_words.split()).value_counts()\n",
    "technology_freq = pd.Series(technology_words.split()).value_counts()\n",
    "entertainment_freq = pd.Series(entertainment_words.split()).value_counts()\n",
    "all_freq = pd.Series(all_words.split()).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a3b6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = pd.concat([science_freq, business_freq, sports_freq, technology_freq, entertainment_freq, all_freq], axis=1).fillna(0).astype(int)\n",
    "word_counts.columns = ['science','business','sports','technology','entertainment','all']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd7e7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb3a2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts['all'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf3ec88",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "Visualize the top 5 words and determine which category uses those words the most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313d4f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('font', size=18)\n",
    "word_counts.sort_values('all', ascending=False).head(5)[['science', 'technology', 'business', 'sports', 'entertainment']].plot.barh()\n",
    "plt.title('Top 10 Words for Science and Technology')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b282e414",
   "metadata": {},
   "source": [
    "# Takeaways\n",
    "**Out of all words in all categories:**\n",
    "* 'said' is the most common word.\n",
    "* Sports news includes 'India' the most.\n",
    "* Business mentions 'crore' the most.\n",
    "* The word 'billion' is used the most in technology articles.\n",
    "* Sports and Entertainment use the word 'added' the most."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188bc97a",
   "metadata": {},
   "source": [
    "## Visualize the top 10 words for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15791494",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1\n",
    "for category in list(word_counts.columns):\n",
    "    plt.rc('font', size=8)\n",
    "    plt.figure(figsize=(8,10))\n",
    "    plt.subplot(3,2,n)\n",
    "    word_counts[category].sort_values(ascending=False).head(10).plot.barh()\n",
    "    plt.title(f'Top 10 Words for {category}')\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c875f90",
   "metadata": {},
   "source": [
    "# Takeaways\n",
    "* 'said' and 'added'(another form of said) are the most commonly used words in news.\n",
    "* The news is a lot of 'He said/She said/They said\"\n",
    "* Actor is mentioned more than actress in entertainment news.\n",
    "* Entertainment likes to mention films and Instagram.\n",
    "* Meta and Facebook dominated technology news.\n",
    "* Sports news is very nationalistic.\n",
    "* NASA and space are mentioned a lot in science news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329d6a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdfeea1",
   "metadata": {},
   "source": [
    "* Find the top 10 longest words for each category.\n",
    "* Find the average word length for each category.\n",
    "* Find words that are unique to each category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed16712a",
   "metadata": {},
   "source": [
    "I want to create a function that will intake any text and analyze the shit out of it. I want a total character count, character count for the longest word, longest lenth sentence(and that printed), average wordlength per sentence, shortest word(& character count), longest word(& character count), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31903a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_text(string):\n",
    "    # Get length of total characters in all cleaned science articles.\n",
    "    total_characters = len(string)\n",
    "    print(f'Total amount of characters: {total_characters}')\n",
    "    \n",
    "    # Get wordcount of all words in cleaned science articles.\n",
    "    total_words = len(string.split())\n",
    "    print(f'Total amount of words: {total_words}')\n",
    "    \n",
    "    # Get list of unique words and a count in cleaned science articles.\n",
    "    unique_words = pd.DataFrame(string.split())[0].unique()\n",
    "    print('Total amount of unique words: ',len(unique_words)\n",
    "          \n",
    "    # Get length of every unique word and plot a histogram of how many times each length of word appears.\n",
    "    sns.histplot([len(word) for word in unique_words], binwidth=1)\n",
    "    plt.xlabel('character_count')\n",
    "    plt.title('Number of Characters in Each Word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39896db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get length of total characters in all cleaned science articles.\n",
    "len(science_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a67f6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get wordcount of all words in cleaned science articles.\n",
    "len(science_words.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d912b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of unique words and a count in cleaned science articles.\n",
    "unique_words = pd.DataFrame(science_words.split())[0].unique()\n",
    "\n",
    "len(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e906b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get length of every unique word and plot a histogram of how many times each length of word appears.\n",
    "sns.histplot([len(word) for word in unique_words], binwidth=1)\n",
    "plt.xlabel('character_count')\n",
    "plt.title('Number of Characters in Each Word')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7479e922",
   "metadata": {},
   "source": [
    "# Takeaways\n",
    "* The distribution of word length is slightly right skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6381836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get average word length of all words in cleaned science articles.\n",
    "pd.Series([len(word) for word in unique_words]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac6a18b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
