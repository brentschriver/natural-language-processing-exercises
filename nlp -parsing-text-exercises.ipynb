{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16f36262",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import acquire\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from acquire import parse_blog\n",
    "from acquire import get_article_text\n",
    "from acquire import get_codeup_blogs\n",
    "from acquire import get_inshorts_articles\n",
    "from acquire import prep_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082d373d",
   "metadata": {},
   "source": [
    "## From acquire, use 'get_article_text' function and store results into variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52aab1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "original = get_article_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d0510d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase everything in the text.\n",
    "article = original.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cb2559",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab70d08",
   "metadata": {},
   "source": [
    "## Remove Accented Characters\n",
    "\n",
    "Convert invalid characters into ASCII characters.\n",
    "1. 'unicodedata.normalize' will remove inconsistencies in unicode character encoding.\n",
    "2. '.encode' will convert the resulting string to the ASCII character set. \n",
    "3. '.decode' turns the resulting bytes object back into a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c117b789",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = unicodedata.normalize('NFKD', article)\\\n",
    "    .encode('ascii', 'ignore')\\\n",
    "    .decode('utf-8', 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ae3864",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8ce26a",
   "metadata": {},
   "source": [
    "## Remove Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2fb484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove anything that isn't a-z, a number, single quote, or whitespace.\n",
    "article = re.sub(r\"[^a-z0-9'\\s]\", '', article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c8edbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee625bc",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "##### Use nltk to tokenize the strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591e50fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.tokenize.ToktokTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61668e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize(original, return_str=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b830000",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization\n",
    "### Stemming\n",
    "Reducing words to its root stem. The root stem may not always be an official word found in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb1ecb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the nltk stemmer object, then use it\n",
    "ps = nltk.porter.PorterStemmer()\n",
    "\n",
    "ps.stem('call'), ps.stem('called'), ps.stem('calling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c264128b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply stemming transformation to all the words in the article.\n",
    "stems = [ps.stem(word) for word in article.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7de79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join each word in 'stems' with a space.\n",
    "article_stemmed = ' '.join(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e448dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(article_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d35a78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(stems).value_counts().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a059a43f",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "The base form of a lemmatized word is the root word(lemma). Lemmas will always be present in dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11be0180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lemmatizer object\n",
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "for word in 'study studies come coming eat eatery eating eaters'.split():\n",
    "    print('stem:', ps.stem(word), '-- lemma:', wnl.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c62163",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [wnl.lemmatize(word) for word in article.split()]\n",
    "article_lemmatized = ' '.join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec4f729",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(article_lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8637e2d",
   "metadata": {},
   "source": [
    "## Removing Stopwords\n",
    "**stopword:** words that have little to no significance while constructing meaningful features from text.\n",
    "* Articles, conjunctions, and prepositions are some examples of stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cc6e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = stopwords.words('english')\n",
    "#stopword_list.remove('no')\n",
    "#stopword_list.remove('not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddca489",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075baf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = article.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2095c109",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_words = [w for w in words if w not in stopword_list]\n",
    "\n",
    "print('Removed {} stopwords'.format(len(words) - len(filtered_words)))\n",
    "print('---')\n",
    "\n",
    "article_without_stopwords = ' '.join(filtered_words)\n",
    "\n",
    "print(article_without_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14520594",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(string):\n",
    "    '''\n",
    "    This function takes in a string and returns the string normalized.\n",
    "    '''\n",
    "    string = unicodedata.normalize('NFKD', string)\\\n",
    "    .encode('ascii', 'ignore')\\\n",
    "    .decode('utf-8', 'ignore')\n",
    "    # Remove anything that isn't a-z, a number, single quote, or whitespace.\n",
    "    string = re.sub(r\"[^a-z0-9'\\s]\", '', string).lower()\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749fcc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(string):\n",
    "    '''\n",
    "    This function takes in a string and returns a tokenized string.\n",
    "    '''\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "    return tokenizer.tokenize(string, return_str=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45313dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(string):\n",
    "    '''\n",
    "    This function takes in a string and returns a string with words stemmed.\n",
    "    '''\n",
    "    # Create the nltk stemmer object, then use it\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    stems = [ps.stem(word) for word in string.split()]\n",
    "    article_stemmed = ' '.join(stems)\n",
    "    return article_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4700396c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(string):\n",
    "    '''\n",
    "    This function takes in a string and returns a string with words lemmatized.\n",
    "    '''\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    lemmas = [wnl.lemmatize(word) for word in string.split()]\n",
    "    article_lemmatized = ' '.join(lemmas)\n",
    "    return article_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfc3fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(string, extra_words = [], exclude_words = []):\n",
    "    '''\n",
    "    This function takes in a string, optional extra_words and exclude_words parameters with default empty lists and returns a string.\n",
    "    '''\n",
    "    # Create a stopword list.\n",
    "    stopword_list = stopwords.words('english')\n",
    "    # Remove 'exclude_words' from stopword_list to keep these in my text.\n",
    "    stopword_list = set(stopword_list) - set(exclude_words)\n",
    "    # Add in 'extra_words' to stopword_list\n",
    "    stopword_list = stopword_list.union(set(extra_words))\n",
    "    # Split words in string.\n",
    "    words = string.split()\n",
    "    # Create a list of words from my string with stopwords removed and assign to variable.\n",
    "    filtered_words = [w for w in words if w not in stopword_list]\n",
    "    print('Removed {} stopwords'.format(len(words) - len(filtered_words)))\n",
    "    print('---')\n",
    "    string_without_stopwords = ' '.join(filtered_words)\n",
    "    return string_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d032556d",
   "metadata": {},
   "outputs": [],
   "source": [
    "codeup_df = get_codeup_blogs(cached=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1adda73",
   "metadata": {},
   "outputs": [],
   "source": [
    "codeup_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed9ea84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a a dataframe with the column 'content' dropped and run it\n",
    "# through the newly created function to see if it performs as \n",
    "# expected.\n",
    "\n",
    "# b = codeup_df.drop(columns='content',inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8062d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_text(codeup_df, 'original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36162d3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "news_df = get_inshorts_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daabcbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663b6e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_text(news_df, 'original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98ca7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdeae66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
